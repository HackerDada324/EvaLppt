{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU acceleration enabled: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "OpenCV was not built with CUDA support, but PyTorch CUDA will be used for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   0%|          | 0/16990 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:  10%|█         | 1775/16990 [07:44<1:06:23,  3.82it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Process a video file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjitro\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mvideoplayback.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 8\u001b[0m stats \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shows a progress bar if tqdm is installed\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Work with the returned stats\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stats:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Access various statistics\u001b[39;00m\n",
      "File \u001b[1;32md:\\Auto_PPT_Evaluation\\posture_analysis\\motion_analyzer\\body_motion.py:286\u001b[0m, in \u001b[0;36mBodyMotionAnalyzer.process_video\u001b[1;34m(self, video_path, sampling_rate, show_progress, batch_size)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, f_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_indices):\n\u001b[0;32m    285\u001b[0m     frame_rgb \u001b[38;5;241m=\u001b[39m processed_frames[i] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_frames, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m processed_frames[i]\n\u001b[1;32m--> 286\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_analyze_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m         frames_with_detection \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32md:\\Auto_PPT_Evaluation\\posture_analysis\\motion_analyzer\\body_motion.py:82\u001b[0m, in \u001b[0;36mBodyMotionAnalyzer._analyze_frame\u001b[1;34m(self, image_rgb)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError configuring TensorFlow: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContinuing with standard MediaPipe configuration\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpose\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\Python_3_10_16\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\Python_3_10_16\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    336\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    337\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    338\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import the analyzer\n",
    "from posture_analysis.motion_analyzer.body_motion import BodyMotionAnalyzer\n",
    "\n",
    "analyzer = BodyMotionAnalyzer(min_detection_confidence=0.7, tilt_threshold=5)\n",
    "\n",
    "# Process a video file\n",
    "video_path = r\"C:\\Users\\jitro\\Downloads\\videoplayback.mp4\"\n",
    "stats = analyzer.process_video(\n",
    "    video_path=video_path,\n",
    "    sampling_rate=1, \n",
    "    show_progress=True  # Shows a progress bar if tqdm is installed\n",
    ")\n",
    "\n",
    "# Work with the returned stats\n",
    "if stats:\n",
    "    # Access various statistics\n",
    "    print(f\"Mean tilt angle: {stats.mean_angle}\")\n",
    "    print(f\"Dominant direction: {stats.dominant_direction}\")\n",
    "    print(f\"Stability score: {stats.stability_score}\")\n",
    "    \n",
    "    # You can access all the properties of the VideoAnalysisStats object\n",
    "    # For example, direction percentages\n",
    "    for direction, percentage in stats.direction_percentages.items():\n",
    "        print(f\"Direction {direction}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   0%|          | 0/16990 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|██████████| 16990/16990 [00:09<00:00, 1709.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Face Tilt Analysis Complete for C:\\Users\\jitro\\Downloads\\videoplayback.mp4\n",
      "- Duration: 566.90 seconds\n",
      "- Frames processed: 11/16990 (25.90%)\n",
      "- Average tilt angle: 4.97° ± 5.10°\n",
      "- Dominant tilt direction: right\n",
      "- Stability score: 5.10 (lower is more stable)\n",
      "- Processing time: 9.94 seconds (1708.61 FPS)\n",
      "Mean tilt angle: 4.969695225585337\n",
      "Dominant direction: right\n",
      "Stability score: 5.096109674659685\n",
      "Direction left: 45.45%\n",
      "Direction right: 54.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from posture_analysis.motion_analyzer.face_motion import FaceMotionAnalyzer\n",
    "\n",
    "analyzer = FaceMotionAnalyzer(min_detection_confidence=0.7, tilt_threshold=5)\n",
    "\n",
    "# Process a video file\n",
    "video_path = r\"C:\\Users\\jitro\\Downloads\\videoplayback.mp4\"\n",
    "stats = analyzer.process_video(\n",
    "    video_path=video_path,\n",
    "    sampling_rate=400, \n",
    "    show_progress=True  # Shows a progress bar if tqdm is installed\n",
    ")\n",
    "\n",
    "# Work with the returned stats\n",
    "if stats:\n",
    "    # Access various statistics\n",
    "    print(f\"Mean tilt angle: {stats.mean_angle}\")\n",
    "    print(f\"Dominant direction: {stats.dominant_direction}\")\n",
    "    print(f\"Stability score: {stats.stability_score}\")\n",
    "    \n",
    "    # You can access all the properties of the VideoAnalysisStats object\n",
    "    # For example, direction percentages\n",
    "    for direction, percentage in stats.direction_percentages.items():\n",
    "        print(f\"Direction {direction}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video:   0%|          | 0/16990 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|██████████| 16990/16990 [01:10<00:00, 241.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaze Analysis Complete for C:\\Users\\jitro\\Downloads\\videoplayback.mp4\n",
      "- Duration: 566.90 seconds\n",
      "- Frames processed: 16/16990 (37.67%)\n",
      "- Dominant gaze direction: left\n",
      "- Eye contact maintained: 0.00% of detected frames\n",
      "- Direction breakdown: {'left': 100.0}\n",
      "- Processing time: 70.36 seconds (241.47 FPS)\n",
      "Dominant gaze direction: left\n",
      "Detection rate: 37.67%\n",
      "Eye contact percentage: 0.00%\n",
      "Direction left: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from posture_analysis.motion_analyzer.gaze_motion import GazeMotionAnalyzer\n",
    "\n",
    "# Initialize the gaze analyzer\n",
    "gaze_analyzer = GazeMotionAnalyzer(min_detection_confidence=0.7)\n",
    "\n",
    "# Process a video file\n",
    "video_path = r\"C:\\Users\\jitro\\Downloads\\videoplayback.mp4\"\n",
    "stats = gaze_analyzer.process_video(\n",
    "    video_path=video_path,\n",
    "    sampling_rate=400,  # You can adjust this based on your needs\n",
    "    show_progress=True  # Shows a progress bar if tqdm is installed\n",
    ")\n",
    "\n",
    "# Work with the returned stats\n",
    "if stats:\n",
    "    # Access various statistics\n",
    "    print(f\"Dominant gaze direction: {stats.dominant_direction}\")\n",
    "    print(f\"Detection rate: {stats.detection_rate:.2f}%\")\n",
    "    \n",
    "    # Access the eye contact percentage from additional_stats\n",
    "    print(f\"Eye contact percentage: {stats.additional_stats['eye_contact_percentage']:.2f}%\")\n",
    "    \n",
    "    # Print direction percentages\n",
    "    for direction, percentage in stats.direction_percentages.items():\n",
    "        print(f\"Direction {direction}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing video: 100%|██████████| 16990/16990 [00:14<00:00, 1163.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shoulder Tilt Analysis Complete for C:\\Users\\jitro\\Downloads\\videoplayback.mp4\n",
      "- Duration: 566.90 seconds\n",
      "- Frames processed: 42/16990 (98.88%)\n",
      "- Average tilt angle: 11.89° ± 15.02°\n",
      "- Dominant tilt direction: none\n",
      "- Stability score: 15.02 (lower is more stable)\n",
      "- Processing time: 14.61 seconds (1162.98 FPS)\n",
      "Mean tilt angle: 11.8881858175514\n",
      "Stability score: 15.021033326637282\n",
      "Dominant tilt direction: none\n",
      "Direction percentages: {'right': 23.809523809523807, 'none': 45.23809523809524, 'left': 30.952380952380953}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from posture_analysis.motion_analyzer.shoulder_motion import ShoulderMotionAnalyzer\n",
    "analyzer = ShoulderMotionAnalyzer(min_detection_confidence=0.5, tilt_threshold=5)\n",
    "video_path = r\"C:\\Users\\jitro\\Downloads\\videoplayback.mp4\"\n",
    "\n",
    "stats = analyzer.process_video(\n",
    "    video_path=video_path,\n",
    "    sampling_rate=400, \n",
    "    show_progress=True \n",
    ")\n",
    "\n",
    "if stats:\n",
    "    print(f\"Mean tilt angle: {stats.mean_angle}\")\n",
    "    print(f\"Stability score: {stats.stability_score}\")\n",
    "    print(f\"Dominant tilt direction: {stats.dominant_direction}\")\n",
    "    print(f\"Direction percentages: {stats.direction_percentages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "CUDA Version: 11.8\n",
      "cuDNN Version: 90100\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"CUDA Version:\", torch.version.cuda)\n",
    "    print(\"cuDNN Version:\", torch.backends.cudnn.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3_10_16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
